Logging to C:\Users\v-liguoq\Desktop\DRL\projects\nips2018_challenge\codebase\guoqing\nipsrun2019-ensemble\ddpg-nips\log\DDPG-Ensemble-2018-10-31-14-32-43-0
Command Line: ['main.py']
setting up actor optimizer
  actor shapes: [[214, 64], [64], [64], [64], [64, 64], [64], [64], [64], [64, 19], [19]]
  actor params: 19411
setting up critic optimizer
  regularizing: critic_0th/dense/kernel:0
  regularizing: critic_0th/dense_1/kernel:0
  regularizing: critic_0th/dense_2/kernel:0
  applying l2 regularization with 0.01
  critic shapes: [[233, 64], [64], [64], [64], [83, 64], [64], [64], [64], [64, 1], [1]]
  critic params: 20673
setting up target updates ...
  target_actor_0th/dense/kernel:0 <- actor_0th/dense/kernel:0
  target_actor_0th/dense/bias:0 <- actor_0th/dense/bias:0
  target_actor_0th/LayerNorm/beta:0 <- actor_0th/LayerNorm/beta:0
  target_actor_0th/LayerNorm/gamma:0 <- actor_0th/LayerNorm/gamma:0
  target_actor_0th/dense_1/kernel:0 <- actor_0th/dense_1/kernel:0
  target_actor_0th/dense_1/bias:0 <- actor_0th/dense_1/bias:0
  target_actor_0th/LayerNorm_1/beta:0 <- actor_0th/LayerNorm_1/beta:0
  target_actor_0th/LayerNorm_1/gamma:0 <- actor_0th/LayerNorm_1/gamma:0
  target_actor_0th/dense_2/kernel:0 <- actor_0th/dense_2/kernel:0
  target_actor_0th/dense_2/bias:0 <- actor_0th/dense_2/bias:0
setting up target updates ...
  target_critic_0th/dense/kernel:0 <- critic_0th/dense/kernel:0
  target_critic_0th/dense/bias:0 <- critic_0th/dense/bias:0
  target_critic_0th/LayerNorm/beta:0 <- critic_0th/LayerNorm/beta:0
  target_critic_0th/LayerNorm/gamma:0 <- critic_0th/LayerNorm/gamma:0
  target_critic_0th/dense_1/kernel:0 <- critic_0th/dense_1/kernel:0
  target_critic_0th/dense_1/bias:0 <- critic_0th/dense_1/bias:0
  target_critic_0th/LayerNorm_1/beta:0 <- critic_0th/LayerNorm_1/beta:0
  target_critic_0th/LayerNorm_1/gamma:0 <- critic_0th/LayerNorm_1/gamma:0
  target_critic_0th/dense_2/kernel:0 <- critic_0th/dense_2/kernel:0
  target_critic_0th/dense_2/bias:0 <- critic_0th/dense_2/bias:0
setting up actor optimizer
  actor shapes: [[214, 64], [64], [64], [64], [64, 64], [64], [64], [64], [64, 19], [19]]
  actor params: 19411
setting up critic optimizer
  regularizing: critic_1th/dense/kernel:0
  regularizing: critic_1th/dense_1/kernel:0
  regularizing: critic_1th/dense_2/kernel:0
  applying l2 regularization with 0.01
  critic shapes: [[233, 64], [64], [64], [64], [83, 64], [64], [64], [64], [64, 1], [1]]
  critic params: 20673
setting up target updates ...
  target_actor_1th/dense/kernel:0 <- actor_1th/dense/kernel:0
  target_actor_1th/dense/bias:0 <- actor_1th/dense/bias:0
  target_actor_1th/LayerNorm/beta:0 <- actor_1th/LayerNorm/beta:0
  target_actor_1th/LayerNorm/gamma:0 <- actor_1th/LayerNorm/gamma:0
  target_actor_1th/dense_1/kernel:0 <- actor_1th/dense_1/kernel:0
  target_actor_1th/dense_1/bias:0 <- actor_1th/dense_1/bias:0
  target_actor_1th/LayerNorm_1/beta:0 <- actor_1th/LayerNorm_1/beta:0
  target_actor_1th/LayerNorm_1/gamma:0 <- actor_1th/LayerNorm_1/gamma:0
  target_actor_1th/dense_2/kernel:0 <- actor_1th/dense_2/kernel:0
  target_actor_1th/dense_2/bias:0 <- actor_1th/dense_2/bias:0
setting up target updates ...
  target_critic_1th/dense/kernel:0 <- critic_1th/dense/kernel:0
  target_critic_1th/dense/bias:0 <- critic_1th/dense/bias:0
  target_critic_1th/LayerNorm/beta:0 <- critic_1th/LayerNorm/beta:0
  target_critic_1th/LayerNorm/gamma:0 <- critic_1th/LayerNorm/gamma:0
  target_critic_1th/dense_1/kernel:0 <- critic_1th/dense_1/kernel:0
  target_critic_1th/dense_1/bias:0 <- critic_1th/dense_1/bias:0
  target_critic_1th/LayerNorm_1/beta:0 <- critic_1th/LayerNorm_1/beta:0
  target_critic_1th/LayerNorm_1/gamma:0 <- critic_1th/LayerNorm_1/gamma:0
  target_critic_1th/dense_2/kernel:0 <- critic_1th/dense_2/kernel:0
  target_critic_1th/dense_2/bias:0 <- critic_1th/dense_2/bias:0
setting up actor optimizer
  actor shapes: [[214, 64], [64], [64], [64], [64, 64], [64], [64], [64], [64, 19], [19]]
  actor params: 19411
setting up critic optimizer
  regularizing: critic_2th/dense/kernel:0
  regularizing: critic_2th/dense_1/kernel:0
  regularizing: critic_2th/dense_2/kernel:0
  applying l2 regularization with 0.01
  critic shapes: [[233, 64], [64], [64], [64], [83, 64], [64], [64], [64], [64, 1], [1]]
  critic params: 20673
setting up target updates ...
  target_actor_2th/dense/kernel:0 <- actor_2th/dense/kernel:0
  target_actor_2th/dense/bias:0 <- actor_2th/dense/bias:0
  target_actor_2th/LayerNorm/beta:0 <- actor_2th/LayerNorm/beta:0
  target_actor_2th/LayerNorm/gamma:0 <- actor_2th/LayerNorm/gamma:0
  target_actor_2th/dense_1/kernel:0 <- actor_2th/dense_1/kernel:0
  target_actor_2th/dense_1/bias:0 <- actor_2th/dense_1/bias:0
  target_actor_2th/LayerNorm_1/beta:0 <- actor_2th/LayerNorm_1/beta:0
  target_actor_2th/LayerNorm_1/gamma:0 <- actor_2th/LayerNorm_1/gamma:0
  target_actor_2th/dense_2/kernel:0 <- actor_2th/dense_2/kernel:0
  target_actor_2th/dense_2/bias:0 <- actor_2th/dense_2/bias:0
setting up target updates ...
  target_critic_2th/dense/kernel:0 <- critic_2th/dense/kernel:0
  target_critic_2th/dense/bias:0 <- critic_2th/dense/bias:0
  target_critic_2th/LayerNorm/beta:0 <- critic_2th/LayerNorm/beta:0
  target_critic_2th/LayerNorm/gamma:0 <- critic_2th/LayerNorm/gamma:0
  target_critic_2th/dense_1/kernel:0 <- critic_2th/dense_1/kernel:0
  target_critic_2th/dense_1/bias:0 <- critic_2th/dense_1/bias:0
  target_critic_2th/LayerNorm_1/beta:0 <- critic_2th/LayerNorm_1/beta:0
  target_critic_2th/LayerNorm_1/gamma:0 <- critic_2th/LayerNorm_1/gamma:0
  target_critic_2th/dense_2/kernel:0 <- critic_2th/dense_2/kernel:0
  target_critic_2th/dense_2/bias:0 <- critic_2th/dense_2/bias:0
setting up actor optimizer
  actor shapes: [[214, 64], [64], [64], [64], [64, 64], [64], [64], [64], [64, 19], [19]]
  actor params: 19411
setting up critic optimizer
  regularizing: critic_3th/dense/kernel:0
  regularizing: critic_3th/dense_1/kernel:0
  regularizing: critic_3th/dense_2/kernel:0
  applying l2 regularization with 0.01
  critic shapes: [[233, 64], [64], [64], [64], [83, 64], [64], [64], [64], [64, 1], [1]]
  critic params: 20673
setting up target updates ...
  target_actor_3th/dense/kernel:0 <- actor_3th/dense/kernel:0
  target_actor_3th/dense/bias:0 <- actor_3th/dense/bias:0
  target_actor_3th/LayerNorm/beta:0 <- actor_3th/LayerNorm/beta:0
  target_actor_3th/LayerNorm/gamma:0 <- actor_3th/LayerNorm/gamma:0
  target_actor_3th/dense_1/kernel:0 <- actor_3th/dense_1/kernel:0
  target_actor_3th/dense_1/bias:0 <- actor_3th/dense_1/bias:0
  target_actor_3th/LayerNorm_1/beta:0 <- actor_3th/LayerNorm_1/beta:0
  target_actor_3th/LayerNorm_1/gamma:0 <- actor_3th/LayerNorm_1/gamma:0
  target_actor_3th/dense_2/kernel:0 <- actor_3th/dense_2/kernel:0
  target_actor_3th/dense_2/bias:0 <- actor_3th/dense_2/bias:0
setting up target updates ...
  target_critic_3th/dense/kernel:0 <- critic_3th/dense/kernel:0
  target_critic_3th/dense/bias:0 <- critic_3th/dense/bias:0
  target_critic_3th/LayerNorm/beta:0 <- critic_3th/LayerNorm/beta:0
  target_critic_3th/LayerNorm/gamma:0 <- critic_3th/LayerNorm/gamma:0
  target_critic_3th/dense_1/kernel:0 <- critic_3th/dense_1/kernel:0
  target_critic_3th/dense_1/bias:0 <- critic_3th/dense_1/bias:0
  target_critic_3th/LayerNorm_1/beta:0 <- critic_3th/LayerNorm_1/beta:0
  target_critic_3th/LayerNorm_1/gamma:0 <- critic_3th/LayerNorm_1/gamma:0
  target_critic_3th/dense_2/kernel:0 <- critic_3th/dense_2/kernel:0
  target_critic_3th/dense_2/bias:0 <- critic_3th/dense_2/bias:0
setting up actor optimizer
  actor shapes: [[214, 64], [64], [64], [64], [64, 64], [64], [64], [64], [64, 19], [19]]
  actor params: 19411
setting up critic optimizer
  regularizing: critic_4th/dense/kernel:0
  regularizing: critic_4th/dense_1/kernel:0
  regularizing: critic_4th/dense_2/kernel:0
  applying l2 regularization with 0.01
  critic shapes: [[233, 64], [64], [64], [64], [83, 64], [64], [64], [64], [64, 1], [1]]
  critic params: 20673
setting up target updates ...
  target_actor_4th/dense/kernel:0 <- actor_4th/dense/kernel:0
  target_actor_4th/dense/bias:0 <- actor_4th/dense/bias:0
  target_actor_4th/LayerNorm/beta:0 <- actor_4th/LayerNorm/beta:0
  target_actor_4th/LayerNorm/gamma:0 <- actor_4th/LayerNorm/gamma:0
  target_actor_4th/dense_1/kernel:0 <- actor_4th/dense_1/kernel:0
  target_actor_4th/dense_1/bias:0 <- actor_4th/dense_1/bias:0
  target_actor_4th/LayerNorm_1/beta:0 <- actor_4th/LayerNorm_1/beta:0
  target_actor_4th/LayerNorm_1/gamma:0 <- actor_4th/LayerNorm_1/gamma:0
  target_actor_4th/dense_2/kernel:0 <- actor_4th/dense_2/kernel:0
  target_actor_4th/dense_2/bias:0 <- actor_4th/dense_2/bias:0
setting up target updates ...
  target_critic_4th/dense/kernel:0 <- critic_4th/dense/kernel:0
  target_critic_4th/dense/bias:0 <- critic_4th/dense/bias:0
  target_critic_4th/LayerNorm/beta:0 <- critic_4th/LayerNorm/beta:0
  target_critic_4th/LayerNorm/gamma:0 <- critic_4th/LayerNorm/gamma:0
  target_critic_4th/dense_1/kernel:0 <- critic_4th/dense_1/kernel:0
  target_critic_4th/dense_1/bias:0 <- critic_4th/dense_1/bias:0
  target_critic_4th/LayerNorm_1/beta:0 <- critic_4th/LayerNorm_1/beta:0
  target_critic_4th/LayerNorm_1/gamma:0 <- critic_4th/LayerNorm_1/gamma:0
  target_critic_4th/dense_2/kernel:0 <- critic_4th/dense_2/kernel:0
  target_critic_4th/dense_2/bias:0 <- critic_4th/dense_2/bias:0
evaluation time: 69.14169025421143
Forward distance: [-0.11332140331977557, -0.11375505313622634, -0.11464380924248875, -0.11332140331977557, -0.11527840608043903, -0.11332140331977557, -0.11375505313622634, -0.11464380924248875, -0.11332140331977557, -0.11527840608043903]
Eval results: [560.7958316981293, 560.0429572837891, 560.2661650963254, 560.4617309159386, 560.2313194317846, 563.6207129628823, 578.2552719586747, 578.9637166116788, 566.9278987467301, 577.836746608408, 558.2642315158464, 558.8475977187669, 559.3235861620005, 558.2642315158464, 558.8248715262007, 564.3993670585319, 564.7039371899, 564.4025749680646, 564.3993670585319, 563.9663115872487, 565.2016783652034, 564.970649543972, 564.6463758608564, 565.2016783652034, 564.2585028170821]
------------------------------------
| epoch/episode         | 521      |
| epoch/reward_sum      | -18.5    |
| epoch/steps           | 29.1     |
| epoch/time            | 2.08e+03 |
| eval/mean_forward     | -0.114   |
| eval/reward_sum       | 564      |
| obs_rms_mean          | 20.3     |
| obs_rms_std           | 143      |
| reference_Q_std       | 122      |
| reference_action_mean | 2.08     |
| reference_action_std  | 2.32     |
| reference_actor_Q_std | 115      |
| total/episode         | 521      |
| total/epochs          | 1        |
| total/steps           | 15360    |
| total/time            | 2.08e+03 |
| train/actor_loss      | 19.5     |
| train/critic_loss     | 6.83     |
------------------------------------

Send graph to server.
Send graph to server.
Send graph to server.
evaluation time: 71.53113293647766
Forward distance: [0.02141480790135846, 0.02183900619242116, 0.022320650537723457, 0.02141480790135846, 0.02103638244998782, 0.02141480790135846, 0.02183900619242116, 0.022320650537723457, 0.02141480790135846, 0.02103638244998782]
Eval results: [914.8024895801419, 914.7989117993505, 914.894208933269, 914.8024895801419, 914.8179002468462, 914.8754910560884, 914.8092755488972, 914.8643857148568, 914.8754910560884, 914.866955220221, 914.9126182117025, 914.9980139283882, 914.7980888327547, 914.9126182117025, 914.8768999981852, 882.0219876050924, 880.8782642359857, 882.016702703422, 882.0219876050924, 882.023370928564, 914.8920145806871, 914.9591787431269, 915.049412873338, 914.8920145806871, 914.8143029950052]
------------------------------------
| epoch/episode         | 410      |
| epoch/reward_sum      | -1.74    |
| epoch/steps           | 37.7     |
| epoch/time            | 3.13e+03 |
| eval/mean_forward     | 0.0216   |
| eval/reward_sum       | 908      |
| obs_rms_mean          | 18.9     |
| obs_rms_std           | 179      |
| reference_Q_std       | 148      |
| reference_action_mean | 2.02     |
| reference_action_std  | 2.29     |
| reference_actor_Q_std | 141      |
| total/episode         | 931      |
| total/epochs          | 2        |
| total/steps           | 30720    |
| total/time            | 5.22e+03 |
| train/actor_loss      | -9.53    |
| train/critic_loss     | 9.85     |
------------------------------------

Send graph to server.
Send graph to server.
Send graph to server.
evaluation time: 73.14518594741821
Forward distance: [0.13528731243298667, 0.15405407151661477, 0.15405407151661477, 0.13528731243298667, 0.1305650189193794, 0.13528731243298667, 0.15405407151661477, 0.15405407151661477, 0.13528731243298667, 0.1305650189193794]
Eval results: [885.2381589124132, 1069.3302829711997, 886.6091630499944, 885.2381589124132, 881.6165947505203, 884.809446977894, 864.113024644405, 862.9305872423251, 884.809446977894, 868.9209128241129, 880.2623309232637, 865.7405120417012, 871.8871362006796, 880.2623309232637, 845.5975757473747, 796.9374027475143, 788.3720781127276, 788.5065810134204, 796.9374027475143, 786.0207659538778, 1093.7584546881283, 895.0855634934528, 895.0855634934528, 1093.7584546881283, 885.6846045328807]
-------------------------------------
| epoch/episode          | 353      |
| epoch/reward_sum       | 16.1     |
| epoch/steps            | 42.9     |
| epoch/time             | 1.76e+03 |
| eval/mean_forward      | 0.142    |
| eval/reward_sum        | 886      |
| obs_rms_mean           | 16.9     |
| obs_rms_std            | 181      |
| reference_Q_std        | 137      |
| reference_action_mean  | 2.05     |
| reference_action_std   | 2.31     |
| reference_actor_Q_mean | 0.166    |
| reference_actor_Q_std  | 129      |
| total/episode          | 1284     |
| total/epochs           | 3        |
| total/steps            | 46080    |
| total/time             | 6.98e+03 |
| train/actor_loss       | -32.5    |
| train/critic_loss      | 10.1     |
-------------------------------------

Send graph to server.
Send graph to server.
Send graph to server.
evaluation time: 125.332186460495
Forward distance: [0.5283591846832536, 0.48763361487960416, 0.48763361487960416, 0.5283591846832536, 0.4853985175463886, 0.5283591846832536, 0.48763361487960416, 0.48763361487960416, 0.5283591846832536, 0.4853985175463886]
Eval results: [1292.110039047431, 1287.3268285686759, 1287.3268285686759, 1292.110039047431, 1284.0554002861904, 1216.1944720751885, 1213.3364950816494, 1213.3364950816494, 1216.1944720751885, 1218.7702112001812, 1145.2931984294298, 1157.2717833304475, 1157.2717833304475, 1145.2931984294298, 1146.3905984780206, 1234.4843359621154, 1237.481337918104, 1237.481337918104, 1234.4843359621154, 1248.5648478255102, 1249.1060684777353, 1233.4583171230026, 1233.4583171230026, 1249.1060684777353, 1248.8453485184116]
------------------------------------
| epoch/episode         | 235      |
| epoch/reward_sum      | 25.3     |
| epoch/steps           | 65.4     |
| epoch/time            | 1.65e+03 |
| eval/mean_forward     | 0.503    |
| eval/reward_sum       | 1.23e+03 |
| obs_rms_mean          | 14.6     |
| obs_rms_std           | 164      |
| reference_Q_std       | 125      |
| reference_action_mean | 2.15     |
| reference_action_std  | 2.35     |
| reference_actor_Q_std | 118      |
| total/episode         | 1519     |
| total/epochs          | 4        |
| total/steps           | 61440    |
| total/time            | 8.63e+03 |
| train/actor_loss      | -42.3    |
| train/critic_loss     | 9.7      |
------------------------------------

Send graph to server.
Send graph to server.
Send graph to server.
